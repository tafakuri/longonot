{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Wav2Vec2 model for speech recognition with Hugging Face and SageMaker\n",
    "\n",
    "## Background\n",
    "\n",
    "Wav2Vec2 is a transformer-based architecture for ASR tasks and was released in September 2020. We show its simplified architecture diagram below. For more details, see the [original paper](https://arxiv.org/abs/2006.11477). The model is composed of a multi-layer convolutional network (CNN) as feature extractor, which takes input audio signal and outputs audio representations, also considered as features. They are fed into a transformer network to generate contextualized representations. This part of training can be self-supervised, it means that the transformer can be trained with a mass of unlabeled speech and learn from them. Then the model is fine-tuned on labeled data with Connectionist Temporal Classification (CTC) algorithm for specific ASR tasks. The base model we use in this post is [Wav2Vec2-Base-960h](https://huggingface.co/facebook/wav2vec2-base-960h), it is fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. \n",
    "<img src=\"images/wav2vec2.png\">\n",
    "\n",
    "Connectionist Temporal Classification (CTC) is character-based algorithm. During the training, it’s able to demarcate each character of the transcription in the speech automatically, so the timeframe alignment is not required between audio signal and transcription. For example, one audio clip says “Hello World”, we don’t need to know in which second word “hello” is located. It saves a lot of labeling effort for ASR use cases. If you are interested in how the algorithm works underneath, see [this article](https://distill.pub/2017/ctc/) for more information.  \n",
    "\n",
    "\n",
    "## Notebook Overview \n",
    "\n",
    "In this notebook, we use [SUPERB \n",
    "(Speech processing Universal PERformance Benchmark) dataset](https://huggingface.co/datasets/superb) that available from Hugging Face Datasets library, and fine-tune the Wav2Vec2 model and deploy it as SageMaker endpoint for real-time inference for an ASR task. \n",
    "<img src=\"images/solution_overview.png\">\n",
    "\n",
    "First of all, we show how to load and preprocess the SUPERB dataset in SageMaker environment in order to obtain tokenizer and feature extractor, which are required for fine-tuning the Wav2Vec2 model. Then we use SageMaker Script Mode for training and inference steps, that allows you to define and use custom training and inference scripts and SageMaker provides supported Hugging Face framework Docker containers. For more information about training and serving Hugging Face models on SageMaker, see Use [Hugging Face with Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html). This functionality is available through the development of Hugging Face [AWS Deep Learning Container (DLC)](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.html). \n",
    "\n",
    "This notebook is tested in both SageMaker Studio and SageMaker Notebook environments. Below shows detailed setup.   \n",
    "- SageMaker Studio: **ml.m5.xlarge** instance with **Data Science** kernel.\n",
    "- SageMaker Notebook: **ml.m5.xlarge** instance with **conda_python3** kernel. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up \n",
    "First, install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.107.0)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.111.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (21.4.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.24.62)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.3.5)\n",
      "Collecting schema\n",
      "  Using cached schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.9)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.20.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (4.12.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.62 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.62)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.8.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.5.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.13)\n",
      "Requirement already satisfied: pox>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.7.6.5)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.62->boto3<2.0,>=1.20.21->sagemaker) (1.26.12)\n",
      "Installing collected packages: schema, sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.107.0\n",
      "    Uninstalling sagemaker-2.107.0:\n",
      "      Successfully uninstalled sagemaker-2.107.0\n",
      "Successfully installed sagemaker-2.111.0 schema-0.7.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers>=4.4.2\n",
      "  Using cached transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (4.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (1.21.6)\n",
      "Collecting huggingface-hub<1.0,>=0.9.0\n",
      "  Using cached huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (4.42.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (2022.8.17)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (6.0)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers>=4.4.2) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers>=4.4.2) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.4.2) (3.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.4.2) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.4.2) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.4.2) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.4.2) (2.0.4)\n",
      "Installing collected packages: tokenizers, packaging, huggingface-hub, transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.1\n",
      "    Uninstalling packaging-20.1:\n",
      "      Successfully uninstalled packaging-20.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.10.0 packaging-21.3 tokenizers-0.12.1 transformers-4.22.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (2022.7.1)\n",
      "Collecting s3fs\n",
      "  Using cached s3fs-2022.8.2-py3-none-any.whl (27 kB)\n",
      "Collecting aiobotocore~=2.4.0\n",
      "  Using cached aiobotocore-2.4.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from s3fs) (3.8.1)\n",
      "Collecting fsspec==2022.8.2\n",
      "  Using cached fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore~=2.4.0->s3fs) (0.10.0)\n",
      "Collecting botocore<1.27.60,>=1.27.59\n",
      "  Using cached botocore-1.27.59-py3-none-any.whl (9.1 MB)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore~=2.4.0->s3fs) (1.11.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (1.26.12)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (1.14.0)\n",
      "Installing collected packages: fsspec, botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.62\n",
      "    Uninstalling botocore-1.27.62:\n",
      "      Successfully uninstalled botocore-1.27.62\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 2.3.4\n",
      "    Uninstalling aiobotocore-2.3.4:\n",
      "      Successfully uninstalled aiobotocore-2.3.4\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 2022.7.1\n",
      "    Uninstalling s3fs-2022.7.1:\n",
      "      Successfully uninstalled s3fs-2022.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.24.62 requires botocore<1.28.0,>=1.27.62, but you have botocore 1.27.59 which is incompatible.\n",
      "awscli 1.25.63 requires botocore==1.27.62, but you have botocore 1.27.59 which is incompatible.\n",
      "awscli 1.25.63 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "awscli 1.25.63 requires rsa<4.8,>=3.1.2, but you have rsa 4.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.4.0 botocore-1.27.59 fsspec-2022.8.2 s3fs-2022.8.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Using cached datasets-2.5.2-py3-none-any.whl (432 kB)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.0)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.8.2)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (2.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: xxhash, tqdm, responses, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.1\n",
      "    Uninstalling tqdm-4.42.1:\n",
      "      Successfully uninstalled tqdm-4.42.1\n",
      "Successfully installed datasets-2.5.2 responses-0.18.0 tqdm-4.64.1 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch\n",
      "  Using cached torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.3.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.12.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchaudio\n",
      "  Using cached torchaudio-0.12.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
      "Requirement already satisfied: torch==1.12.1 in /opt/conda/lib/python3.7/site-packages (from torchaudio) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.1->torchaudio) (4.3.0)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-0.12.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - librosa\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    appdirs-1.4.4              |     pyh9f0ad1d_0          13 KB  conda-forge\n",
      "    audioread-3.0.0            |   py37h89c1867_0          34 KB  conda-forge\n",
      "    ca-certificates-2022.9.24  |       ha878542_0         150 KB  conda-forge\n",
      "    certifi-2022.9.24          |     pyhd8ed1ab_0         155 KB  conda-forge\n",
      "    conda-22.9.0               |   py37h89c1867_1         960 KB  conda-forge\n",
      "    ffmpeg-4.2                 |       h167e202_0        80.2 MB  conda-forge\n",
      "    gettext-0.19.8.1           |       h5e8e0c9_1         3.5 MB  conda-forge\n",
      "    gnutls-3.6.13              |       h85f3911_1         2.0 MB  conda-forge\n",
      "    lame-3.100                 |    h14c3975_1001         498 KB  conda-forge\n",
      "    libflac-1.3.3              |       he1b5a44_0         517 KB  conda-forge\n",
      "    libiconv-1.16              |       h516909a_0         1.4 MB  conda-forge\n",
      "    libogg-1.3.2               |    h516909a_1002         206 KB  conda-forge\n",
      "    librosa-0.8.1              |     pyhd8ed1ab_0         147 KB  conda-forge\n",
      "    libsndfile-1.0.29          |       he1b5a44_0         534 KB  conda-forge\n",
      "    libvorbis-1.3.7            |       he1b5a44_0         287 KB  conda-forge\n",
      "    nettle-3.6                 |       he412f7d_0         6.5 MB  conda-forge\n",
      "    openh264-1.8.0             |    hdbcaa40_1000         1.4 MB  conda-forge\n",
      "    pooch-1.6.0                |     pyhd8ed1ab_0          44 KB  conda-forge\n",
      "    pysoundfile-0.11.0         |     pyhd8ed1ab_0          25 KB  conda-forge\n",
      "    resampy-0.2.2              |             py_0         332 KB  conda-forge\n",
      "    x264-1!152.20180806        |       h14c3975_0         1.4 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       100.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  appdirs            conda-forge/noarch::appdirs-1.4.4-pyh9f0ad1d_0\n",
      "  audioread          conda-forge/linux-64::audioread-3.0.0-py37h89c1867_0\n",
      "  ffmpeg             conda-forge/linux-64::ffmpeg-4.2-h167e202_0\n",
      "  gettext            conda-forge/linux-64::gettext-0.19.8.1-h5e8e0c9_1\n",
      "  gnutls             conda-forge/linux-64::gnutls-3.6.13-h85f3911_1\n",
      "  lame               conda-forge/linux-64::lame-3.100-h14c3975_1001\n",
      "  libflac            conda-forge/linux-64::libflac-1.3.3-he1b5a44_0\n",
      "  libiconv           conda-forge/linux-64::libiconv-1.16-h516909a_0\n",
      "  libogg             conda-forge/linux-64::libogg-1.3.2-h516909a_1002\n",
      "  librosa            conda-forge/noarch::librosa-0.8.1-pyhd8ed1ab_0\n",
      "  libsndfile         conda-forge/linux-64::libsndfile-1.0.29-he1b5a44_0\n",
      "  libvorbis          conda-forge/linux-64::libvorbis-1.3.7-he1b5a44_0\n",
      "  nettle             conda-forge/linux-64::nettle-3.6-he412f7d_0\n",
      "  openh264           conda-forge/linux-64::openh264-1.8.0-hdbcaa40_1000\n",
      "  pooch              conda-forge/noarch::pooch-1.6.0-pyhd8ed1ab_0\n",
      "  pysoundfile        conda-forge/noarch::pysoundfile-0.11.0-pyhd8ed1ab_0\n",
      "  resampy            conda-forge/noarch::resampy-0.2.2-py_0\n",
      "  x264               conda-forge/linux-64::x264-1!152.20180806-h14c3975_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2022.6.15-ha878542_0 --> 2022.9.24-ha878542_0\n",
      "  certifi                            2022.6.15-pyhd8ed1ab_1 --> 2022.9.24-pyhd8ed1ab_0\n",
      "  conda                               4.14.0-py37h89c1867_0 --> 22.9.0-py37h89c1867_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "appdirs-1.4.4        | 13 KB     | ##################################### | 100% \n",
      "librosa-0.8.1        | 147 KB    | ##################################### | 100% \n",
      "libsndfile-1.0.29    | 534 KB    | ##################################### | 100% \n",
      "gettext-0.19.8.1     | 3.5 MB    | ##################################### | 100% \n",
      "gnutls-3.6.13        | 2.0 MB    | ##################################### | 100% \n",
      "libogg-1.3.2         | 206 KB    | ##################################### | 100% \n",
      "openh264-1.8.0       | 1.4 MB    | ##################################### | 100% \n",
      "x264-1!152.20180806  | 1.4 MB    | ##################################### | 100% \n",
      "nettle-3.6           | 6.5 MB    | ##################################### | 100% \n",
      "certifi-2022.9.24    | 155 KB    | ##################################### | 100% \n",
      "ffmpeg-4.2           | 80.2 MB   | ##################################### | 100% \n",
      "pysoundfile-0.11.0   | 25 KB     | ##################################### | 100% \n",
      "pooch-1.6.0          | 44 KB     | ##################################### | 100% \n",
      "ca-certificates-2022 | 150 KB    | ##################################### | 100% \n",
      "resampy-0.2.2        | 332 KB    | ##################################### | 100% \n",
      "libflac-1.3.3        | 517 KB    | ##################################### | 100% \n",
      "libvorbis-1.3.7      | 287 KB    | ##################################### | 100% \n",
      "libiconv-1.16        | 1.4 MB    | ##################################### | 100% \n",
      "audioread-3.0.0      | 34 KB     | ##################################### | 100% \n",
      "conda-22.9.0         | 960 KB    | ##################################### | 100% \n",
      "lame-3.100           | 498 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker --upgrade\n",
    "!pip install \"transformers>=4.4.2\" \n",
    "!pip install s3fs --upgrade\n",
    "!pip install datasets --upgrade \n",
    "#!pip install \"librosa==0.9.1librosa\"\n",
    "!pip install torch # framework is required for transformer \n",
    "!pip install torchaudio\n",
    "!pip install transformers\n",
    "!pip install accelerate>=0.5.0\n",
    "!pip install tensorboard\n",
    "\n",
    "\n",
    "!conda install -y -c conda-forge librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**soundfile** library will be used to read raw audio files and convert them into arrays. Before installing **soundfile** python library, package **libsndfile** needs to be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.7/site-packages (0.11.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile) (2.19)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge libsndfile -y\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: librosa 0.9.2\n",
      "Uninstalling librosa-0.9.2:\n",
      "  Successfully uninstalled librosa-0.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall -y librosa\n",
    "#!conda install -y -c conda-forge librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following let's import common python libraries. Create a S3 bucket in AWS console for this project, and replace **[BUCKET_NAME]** with your bucket. \n",
    "Get the execution role which allows training and servering jobs to access your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::121713061542:role/service-role/AmazonSageMaker-ExecutionRole-20220927T193257\n",
      "sagemaker bucket: fine-tune-xlsr-53-wav2vec2-on-swahili\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import random\n",
    "import soundfile \n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "\n",
    "BUCKET=\"fine-tune-xlsr-53-wav2vec2-on-swahili\" # please use your bucket name\n",
    "PREFIX = \"huggingface-blog\" \n",
    "ROLE = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=BUCKET)\n",
    "\n",
    "print(f\"sagemaker role arn: {ROLE}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ea38010a6247629f667699fb08ab3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "We are using SUPERB dataset for this notebook, which can be loaded from Hugging Face [dataset library](https://huggingface.co/datasets/superb) directly using `load_dataset` function. SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. It also includes speaker_id and chapter_id etc., these columns are removed from the dataset, and we only keep audio files and transcriptions to fine-tune the Wav2Vec2 model for an audio recognition task, which transcribes speech to text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "HF_API_TOKEN=HfFolder.get_token()\n",
    "HF_MODEL_ID=\"pretrain-wav2vec2-on-swahili-sagemaker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the HuggingFace model (Wav2Vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "Here we are using SageMaker HuggingFace DLC (Deep Learning Container) script mode to construct the training and inference job, which allows you to write custom trianing and serving code and using HuggingFace framework containers that maintained and supported by AWS. \n",
    "\n",
    "When we create a training job using the script mode, the `entry_point` script, hyperparameters, its dependencies (inside requirements.txt) and input data (train and test datasets) will be copied into the container. Then it invokes the `entry_point` training script, where the train and test datasets will be loaded, training steps will be executed and model artifacts will be saved in `/opt/ml/model` in the container. After training, artifacts in this directory are uploaded to S3 for later model hosting.\n",
    "\n",
    "This script is saved in directory `scripts`, and you can inspect the training script by running the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    Wav2Vec2ForCTC, \n",
      "    Trainer, \n",
      "    TrainingArguments, \n",
      "    Wav2Vec2CTCTokenizer, \n",
      "    Wav2Vec2FeatureExtractor, \n",
      "    Wav2Vec2Processor)\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk, load_metric\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdataclasses\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m dataclass\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dict, List, Optional, Union\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      " \n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m8\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m8\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mfacebook/wav2vec2-base\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m1e-4\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--weight_decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m0.005\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--vocab_url\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[37m# set up logging\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "    logging.basicConfig(\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(args.training_dir)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args.test_dir)\n",
      "    train_dataset = load_from_disk(args.training_dir)\n",
      "    test_dataset = load_from_disk(args.test_dir)\n",
      "\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(train_dataset)}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(test_dataset)}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "\n",
      "    \u001b[37m# add tokenizer, feature extractor and processor \u001b[39;49;00m\n",
      "    s3 = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    vocab_url = args.vocab_url\n",
      "    s3.download_file(vocab_url.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m2\u001b[39;49;00m],vocab_url.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m3\u001b[39;49;00m]+\u001b[33m'\u001b[39;49;00m\u001b[33m/vocab.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mvocab.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    tokenizer = Wav2Vec2CTCTokenizer(\u001b[33m\"\u001b[39;49;00m\u001b[33mvocab.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, unk_token=\u001b[33m\"\u001b[39;49;00m\u001b[33m[UNK]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, pad_token=\u001b[33m\"\u001b[39;49;00m\u001b[33m[PAD]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, word_delimiter_token=\u001b[33m\"\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \n",
      "    feature_extractor = Wav2Vec2FeatureExtractor(feature_size=\u001b[34m1\u001b[39;49;00m, sampling_rate=\u001b[34m16000\u001b[39;49;00m, padding_value=\u001b[34m0.0\u001b[39;49;00m, do_normalize=\u001b[34mTrue\u001b[39;49;00m, return_attention_mask=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
      "\n",
      "    \u001b[37m# the data collator class is for dynamic padding and speed up data processing in CPU GPU. The code is from: \u001b[39;49;00m\n",
      "    \u001b[37m# https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81\u001b[39;49;00m\n",
      "    \u001b[90m@dataclass\u001b[39;49;00m\n",
      "    \u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDataCollatorCTCWithPadding\u001b[39;49;00m:\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Data collator that will dynamically pad the inputs received.\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            processor (:class:`~transformers.Wav2Vec2Processor`)\u001b[39;49;00m\n",
      "\u001b[33m                The processor used for proccessing the data.\u001b[39;49;00m\n",
      "\u001b[33m            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\u001b[39;49;00m\n",
      "\u001b[33m                Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\u001b[39;49;00m\n",
      "\u001b[33m                among:\u001b[39;49;00m\n",
      "\u001b[33m                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\u001b[39;49;00m\n",
      "\u001b[33m                  sequence if provided).\u001b[39;49;00m\n",
      "\u001b[33m                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\u001b[39;49;00m\n",
      "\u001b[33m                  maximum acceptable input length for the model if that argument is not provided.\u001b[39;49;00m\n",
      "\u001b[33m                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\u001b[39;49;00m\n",
      "\u001b[33m                  different lengths).\u001b[39;49;00m\n",
      "\u001b[33m            max_length (:obj:`int`, `optional`):\u001b[39;49;00m\n",
      "\u001b[33m                Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\u001b[39;49;00m\n",
      "\u001b[33m            max_length_labels (:obj:`int`, `optional`):\u001b[39;49;00m\n",
      "\u001b[33m                Maximum length of the ``labels`` returned list and optionally padding length (see above).\u001b[39;49;00m\n",
      "\u001b[33m            pad_to_multiple_of (:obj:`int`, `optional`):\u001b[39;49;00m\n",
      "\u001b[33m                If set will pad the sequence to a multiple of the provided value.\u001b[39;49;00m\n",
      "\u001b[33m                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\u001b[39;49;00m\n",
      "\u001b[33m                7.5 (Volta).\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        processor: Wav2Vec2Processor\n",
      "        padding: Union[\u001b[36mbool\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m] = \u001b[34mTrue\u001b[39;49;00m\n",
      "        max_length: Optional[\u001b[36mint\u001b[39;49;00m] = \u001b[34mNone\u001b[39;49;00m\n",
      "        max_length_labels: Optional[\u001b[36mint\u001b[39;49;00m] = \u001b[34mNone\u001b[39;49;00m\n",
      "        pad_to_multiple_of: Optional[\u001b[36mint\u001b[39;49;00m] = \u001b[34mNone\u001b[39;49;00m\n",
      "        pad_to_multiple_of_labels: Optional[\u001b[36mint\u001b[39;49;00m] = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32m__call__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, features: List[Dict[\u001b[36mstr\u001b[39;49;00m, Union[List[\u001b[36mint\u001b[39;49;00m], torch.Tensor]]]) -> Dict[\u001b[36mstr\u001b[39;49;00m, torch.Tensor]:\n",
      "            \u001b[37m# split inputs and labels since they have to be of different lenghts and need\u001b[39;49;00m\n",
      "            \u001b[37m# different padding methods\u001b[39;49;00m\n",
      "            input_features = [{\u001b[33m\"\u001b[39;49;00m\u001b[33minput_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: feature[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]} \u001b[34mfor\u001b[39;49;00m feature \u001b[35min\u001b[39;49;00m features]\n",
      "            label_features = [{\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: feature[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]} \u001b[34mfor\u001b[39;49;00m feature \u001b[35min\u001b[39;49;00m features]\n",
      "\n",
      "            batch = \u001b[36mself\u001b[39;49;00m.processor.pad(\n",
      "                input_features,\n",
      "                padding=\u001b[36mself\u001b[39;49;00m.padding,\n",
      "                max_length=\u001b[36mself\u001b[39;49;00m.max_length,\n",
      "                pad_to_multiple_of=\u001b[36mself\u001b[39;49;00m.pad_to_multiple_of,\n",
      "                return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            )\n",
      "            \u001b[34mwith\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.processor.as_target_processor():\n",
      "                labels_batch = \u001b[36mself\u001b[39;49;00m.processor.pad(\n",
      "                    label_features,\n",
      "                    padding=\u001b[36mself\u001b[39;49;00m.padding,\n",
      "                    max_length=\u001b[36mself\u001b[39;49;00m.max_length_labels,\n",
      "                    pad_to_multiple_of=\u001b[36mself\u001b[39;49;00m.pad_to_multiple_of_labels,\n",
      "                    return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                )\n",
      "\n",
      "            \u001b[37m# replace padding with -100 to ignore loss correctly\u001b[39;49;00m\n",
      "            labels = labels_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].masked_fill(labels_batch.attention_mask.ne(\u001b[34m1\u001b[39;49;00m), -\u001b[34m100\u001b[39;49;00m)\n",
      "\n",
      "            batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = labels\n",
      "\n",
      "            \u001b[34mreturn\u001b[39;49;00m batch\n",
      "    \n",
      "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# compute metrics\u001b[39;49;00m\n",
      "    wer_metric = load_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33mwer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred):\n",
      "        pred_logits = pred.predictions\n",
      "        pred_ids = np.argmax(pred_logits, axis=-\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "        pred.label_ids[pred.label_ids == -\u001b[34m100\u001b[39;49;00m] = processor.tokenizer.pad_token_id\n",
      "\n",
      "        pred_str = processor.batch_decode(pred_ids)\n",
      "        label_str = processor.batch_decode(pred.label_ids, group_tokens=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mwer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: wer}\n",
      "\n",
      "\n",
      "    model = Wav2Vec2ForCTC.from_pretrained(\n",
      "        args.model_name, \n",
      "        gradient_checkpointing=\u001b[34mTrue\u001b[39;49;00m, \n",
      "        ctc_loss_reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "        pad_token_id=processor.tokenizer.pad_token_id,\n",
      "    )\n",
      "    model.freeze_feature_extractor()\n",
      "\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=args.model_dir,\n",
      "        group_by_length=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        per_device_train_batch_size=args.train_batch_size,\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        num_train_epochs=args.epochs,\n",
      "        fp16=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m# enable mixed-precision training\u001b[39;49;00m\n",
      "        save_steps=\u001b[34m50\u001b[39;49;00m,\n",
      "        eval_steps=\u001b[34m50\u001b[39;49;00m,\n",
      "        logging_steps=\u001b[34m50\u001b[39;49;00m,\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\n",
      "        weight_decay=\u001b[36mfloat\u001b[39;49;00m(args.weight_decay),\n",
      "        warmup_steps=args.warmup_steps,\n",
      "        save_total_limit=\u001b[34m2\u001b[39;49;00m,\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{args.output_data_dir}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        data_collator=data_collator,\n",
      "        args=training_args,\n",
      "        compute_metrics=compute_metrics,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=test_dataset,\n",
      "        tokenizer=processor.feature_extractor,\n",
      "    )\n",
      "    \n",
      "    \u001b[37m# train model\u001b[39;49;00m\n",
      "    trainer.train()\n",
      "\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{key}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{value}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir)\n",
      "    tokenizer.save_pretrained(args.model_dir)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize sagemaker/pretrain_wav2vec/pytorch/run_wav2vec2_pretraining_no_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Estimator and start a training job\n",
    "\n",
    "Worth to highlight that, when you create a Hugging Face Estimator, you can configure hyperparameters and provide a custom parameter into the training script, such as `vocab_url` in this example. Also you can specify the metrics in the Estimator, and parse the logs of metrics and send them to CloudWatch to monitor and track the training performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name:  huggingface-wav2vec2-training-3-1665214372\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "#create an unique id to tag training job, model name and endpoint name. \n",
    "id = int(time.time())\n",
    "\n",
    "TRAINING_JOB_NAME = f\"huggingface-wav2vec2-pretrain-bbc-{id}\"\n",
    "print('Training job name: ', TRAINING_JOB_NAME)\n",
    "\n",
    "vocab_url = f\"s3://{BUCKET}/{PREFIX}/vocab.json\"\n",
    "hyperparameters = {\n",
    "    'dataset_name':\"mutisya/bbc_v0\",\n",
    "    'dataset_split_names': \"train\", \n",
    "    'dataset_config_names': \"train\", \n",
    "\t'dataset_use_auth_token':True,\n",
    "\t'model_name_or_path': \"patrickvonplaten/wav2vec2-base-v2\",\n",
    "\t'output_dir': \"./wav2vec2-pretrain-bbc-demo-4\",\n",
    "\t'max_train_steps': \"20000\",\n",
    "\t'num_warmup_steps': \"32000\",\n",
    "\t'saving_steps': \"10000\",\n",
    "\t'gradient_accumulation_steps': \"4\",\n",
    "\t'learning_rate': \"0.002\",\n",
    "\t'weight_decay' : \"0.01\",\n",
    "\t'max_duration_in_seconds': \"20.0\",\n",
    "\t'min_duration_in_seconds' : \"2.0\",\n",
    "\t'logging_steps': \"1\",\n",
    "\t'per_device_train_batch_size' : \"4\",\n",
    "\t'per_device_eval_batch_size': \"4\",\n",
    "\t'adam_beta1': \"0.9\",\n",
    "\t'adam_beta2' : \"0.98\",\n",
    "\t'adam_epsilon' : \"1e-06\",\n",
    "    'push_to_hub': True, \n",
    "    'gradient_checkpointing': True\n",
    "  }\n",
    "\n",
    "# define metrics definitions\n",
    "metric_definitions=[\n",
    "        {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'eval_wer', 'Regex': \"'eval_wer': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'eval_runtime', 'Regex': \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'eval_samples_per_second', 'Regex': \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an unique id to tag training job, model name and endpoint name. \n",
    "id = int(time.time())\n",
    "\n",
    "TRAINING_JOB_NAME = f\"huggingface-wav2vec2-pretrain-bbc-{id}\"\n",
    "print('Training job name: ', TRAINING_JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [HuggingFace estimator class](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) to train our model. When creating the estimator, the following parameters need to specify. \n",
    "\n",
    "* **entry_point**: the name of the training script. It loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model. \n",
    "* **source_dir**: the location of the training scripts. \n",
    "* **transformers_version**: the Hugging Face transformers library version we want to use.\n",
    "* **pytorch_version**: the pytorch version that compatible with transformers library. \n",
    "\n",
    "**Instance Selection**: For this use case and dataset, we use one ml.p3.2xlarge instance and the training job is able to finish within two hours. You can select a more powerful instance to reduce the training time, however it will generate more cost.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-08 07:35:58 Starting - Starting the training job...\n",
      "2022-10-08 07:36:22 Starting - Preparing the instances for trainingProfilerReport-1665214558: InProgress\n",
      "......\n",
      "2022-10-08 07:37:22 Downloading - Downloading input data....................................\n",
      "2022-10-08 07:43:24 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:14,897 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:14,930 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:14,937 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:15,371 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting jiwer\n",
      "  Downloading jiwer-2.3.0-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-Levenshtein==0.12.2\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from python-Levenshtein==0.12.2->jiwer->-r requirements.txt (line 1)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for python-Levenshtein (setup.py): finished with status 'done'\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp36-cp36m-linux_x86_64.whl size=170614 sha256=db5adb7c70e30d72b075f8aac2320232d592ba3b8e4156bd7f78506a6f8ad27b\n",
      "  Stored in directory: /root/.cache/pip/wheels/4a/a4/bf/d761b0899395c75fa76d003d607b3869ee47f5035b8afc30a2\u001b[0m\n",
      "\u001b[34mSuccessfully built python-Levenshtein\u001b[0m\n",
      "\u001b[34mInstalling collected packages: python-Levenshtein, jiwer\u001b[0m\n",
      "\u001b[34mSuccessfully installed jiwer-2.3.0 python-Levenshtein-0.12.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:21,632 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"fp16\": true,\n",
      "        \"hub_model_id\": \"fine-tune-xlsr-53-wav2vec2-on-swahili-sagemaker\",\n",
      "        \"hub_strategy\": \"all_checkpoints\",\n",
      "        \"hub_token\": \"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\",\n",
      "        \"model_name\": \"facebook/wav2vec2-base\",\n",
      "        \"push_to_hub\": true,\n",
      "        \"train_batch_size\": 8,\n",
      "        \"vocab_url\": \"s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-blog/vocab.json\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-wav2vec2-training-3-1665214372\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-wav2vec2-training-3-1665214372/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"fp16\":true,\"hub_model_id\":\"fine-tune-xlsr-53-wav2vec2-on-swahili-sagemaker\",\"hub_strategy\":\"all_checkpoints\",\"hub_token\":\"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\",\"model_name\":\"facebook/wav2vec2-base\",\"push_to_hub\":true,\"train_batch_size\":8,\"vocab_url\":\"s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-blog/vocab.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-wav2vec2-training-3-1665214372/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"fp16\":true,\"hub_model_id\":\"fine-tune-xlsr-53-wav2vec2-on-swahili-sagemaker\",\"hub_strategy\":\"all_checkpoints\",\"hub_token\":\"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\",\"model_name\":\"facebook/wav2vec2-base\",\"push_to_hub\":true,\"train_batch_size\":8,\"vocab_url\":\"s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-blog/vocab.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-wav2vec2-training-3-1665214372\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-wav2vec2-training-3-1665214372/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--fp16\",\"True\",\"--hub_model_id\",\"fine-tune-xlsr-53-wav2vec2-on-swahili-sagemaker\",\"--hub_strategy\",\"all_checkpoints\",\"--hub_token\",\"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\",\"--model_name\",\"facebook/wav2vec2-base\",\"--push_to_hub\",\"True\",\"--train_batch_size\",\"8\",\"--vocab_url\",\"s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-blog/vocab.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mSM_HP_HUB_MODEL_ID=fine-tune-xlsr-53-wav2vec2-on-swahili-sagemaker\u001b[0m\n",
      "\u001b[34mSM_HP_HUB_STRATEGY=all_checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_HUB_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=facebook/wav2vec2-base\u001b[0m\n",
      "\u001b[34mSM_HP_PUSH_TO_HUB=true\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_VOCAB_URL=s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-blog/vocab.json\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --epochs 10 --fp16 True --hub_model_id fine-tune-xlsr-53-wav2vec2-on-swahili-sagemaker --hub_strategy all_checkpoints --hub_token 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 --model_name facebook/wav2vec2-base --push_to_hub True --train_batch_size 8 --vocab_url s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-blog/vocab.json\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:26,931 - __main__ - INFO -  loaded train_dataset length is: 28869\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:26,932 - __main__ - INFO -  loaded test_dataset length is: 8934\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:28,123 - filelock - INFO - Lock 140441408936704 acquired on /root/.cache/huggingface/transformers/c7746642f045322fd01afa31271dd490e677ea11999e68660a92619ec7c892b4.ce1f96bfaf3d7475cb8187b9668c7f19437ade45fb9ceb78d2b06a2cec198015.lock\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:28,391 - filelock - INFO - Lock 140441408936704 released on /root/.cache/huggingface/transformers/c7746642f045322fd01afa31271dd490e677ea11999e68660a92619ec7c892b4.ce1f96bfaf3d7475cb8187b9668c7f19437ade45fb9ceb78d2b06a2cec198015.lock\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:28,671 - filelock - INFO - Lock 140441408934352 acquired on /root/.cache/huggingface/transformers/ef45231897ce572a660ebc5a63d3702f1a6041c4c5fb78cbec330708531939b3.fcae05302a685f7904c551c8ea571e8bc2a2c4a1777ea81ad66e47f7883a650a.lock\u001b[0m\n",
      "\u001b[34m2022-10-08 07:43:34,787 - filelock - INFO - Lock 140441408934352 released on /root/.cache/huggingface/transformers/ef45231897ce572a660ebc5a63d3702f1a6041c4c5fb78cbec330708531939b3.fcae05302a685f7904c551c8ea571e8bc2a2c4a1777ea81ad66e47f7883a650a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_q.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.043 algo-1:45 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.209 algo-1:45 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.209 algo-1:45 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.210 algo-1:45 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.211 algo-1:45 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.211 algo-1:45 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.939 algo-1:45 INFO hook.py:591] name:wav2vec2.masked_spec_embed count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.940 algo-1:45 INFO hook.py:591] name:wav2vec2.feature_projection.layer_norm.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.940 algo-1:45 INFO hook.py:591] name:wav2vec2.feature_projection.layer_norm.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.940 algo-1:45 INFO hook.py:591] name:wav2vec2.feature_projection.projection.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.940 algo-1:45 INFO hook.py:591] name:wav2vec2.feature_projection.projection.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.940 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.pos_conv_embed.conv.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.940 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.pos_conv_embed.conv.weight_g count_params:128\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.940 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.pos_conv_embed.conv.weight_v count_params:4718592\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.940 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.940 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.941 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.0.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.1.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.942 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.2.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.943 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.944 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.945 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.945 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.945 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.3.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.945 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.945 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.945 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.945 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.945 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.4.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.946 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.947 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.5.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.948 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.6.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.949 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.950 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.950 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.950 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.950 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.950 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.950 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.950 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.7.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.951 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.8.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.952 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.953 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.953 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.953 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.953 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.953 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.953 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.9.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.954 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.955 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.955 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.955 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.955 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.955 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.955 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.955 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.955 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.956 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.956 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.956 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.956 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.10.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.956 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.attention.k_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.956 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.attention.k_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.956 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.attention.v_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.956 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.attention.v_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.956 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.attention.q_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.957 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.attention.q_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.957 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.attention.out_proj.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.957 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.attention.out_proj.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.957 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.957 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.957 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.957 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.957 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.feed_forward.output_dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.957 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.feed_forward.output_dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.958 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.958 algo-1:45 INFO hook.py:591] name:wav2vec2.encoder.layers.11.final_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.958 algo-1:45 INFO hook.py:591] name:lm_head.weight count_params:24576\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.958 algo-1:45 INFO hook.py:591] name:lm_head.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.958 algo-1:45 INFO hook.py:593] Total Trainable Params: 90195872\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.958 algo-1:45 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-10-08 08:00:14.959 algo-1:45 INFO hook.py:488] Hook is writing from the hook with pid: 45\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.95k [00:00<?, ?B/s]#015Downloading: 4.55kB [00:00, 3.12MB/s]                   \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.84k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 1.84k/1.84k [00:00<00:00, 1.20MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/380M [00:00<?, ?B/s]#015Downloading:   1%|          | 4.64M/380M [00:00<00:08, 46.4MB/s]#015Downloading:   3%|▎         | 9.83M/380M [00:00<00:07, 47.9MB/s]#015Downloading:   4%|▍         | 16.3M/380M [00:00<00:07, 51.9MB/s]#015Downloading:   6%|▌         | 22.4M/380M [00:00<00:06, 54.4MB/s]#015Downloading:   8%|▊         | 30.0M/380M [00:00<00:05, 59.5MB/s]#015Downloading:  10%|▉         | 37.3M/380M [00:00<00:05, 62.8MB/s]#015Downloading:  12%|█▏        | 44.9M/380M [00:00<00:05, 66.4MB/s]#015Downloading:  14%|█▍        | 52.9M/380M [00:00<00:04, 69.9MB/s]#015Downloading:  16%|█▌        | 60.4M/380M [00:00<00:04, 71.5MB/s]#015Downloading:  18%|█▊        | 67.5M/380M [00:01<00:04, 68.5MB/s]#015Downloading:  20%|█▉        | 74.9M/380M [00:01<00:04, 70.1MB/s]#015Downloading:  22%|██▏       | 81.9M/380M [00:01<00:04, 69.2MB/s]#015Downloading:  23%|██▎       | 88.8M/380M [00:01<00:06, 46.1MB/s]#015Downloading:  25%|██▍       | 94.5M/380M [00:01<00:06, 42.5MB/s]#015Downloading:  26%|██▌       | 99.7M/380M [00:01<00:06, 45.0MB/s]#015Downloading:  28%|██▊       | 106M/380M [00:01<00:05, 48.6MB/s] #015Downloading:  29%|██▉       | 112M/380M [00:01<00:05, 51.5MB/s]#015Downloading:  31%|███       | 117M/380M [00:02<00:05, 51.5MB/s]#015Downloading:  32%|███▏      | 123M/380M [00:02<00:04, 53.1MB/s]#015Downloading:  34%|███▍      | 128M/380M [00:02<00:04, 52.1MB/s]#015Downloading:  35%|███▌      | 134M/380M [00:02<00:04, 53.2MB/s]#015Downloading:  37%|███▋      | 139M/380M [00:02<00:04, 52.5MB/s]#015Downloading:  38%|███▊      | 146M/380M [00:02<00:04, 56.4MB/s]#015Downloading:  41%|████      | 155M/380M [00:02<00:03, 62.6MB/s]#015Downloading:  42%|████▏     | 161M/380M [00:02<00:03, 64.2MB/s]#015Downloading:  44%|████▍     | 168M/380M [00:02<00:03, 62.9MB/s]#015Downloading:  46%|████▌     | 175M/380M [00:02<00:03, 62.1MB/s]#015Downloading:  48%|████▊     | 182M/380M [00:03<00:03, 65.7MB/s]#015Downloading:  50%|████▉     | 189M/380M [00:03<00:02, 64.7MB/s]#015Downloading:  51%|█████▏    | 196M/380M [00:03<00:02, 65.4MB/s]#015Downloading:  53%|█████▎    | 202M/380M [00:03<00:02, 62.3MB/s]#015Downloading:  55%|█████▍    | 209M/380M [00:03<00:02, 62.9MB/s]#015Downloading:  57%|█████▋    | 215M/380M [00:03<00:02, 59.5MB/s]#015Downloading:  58%|█████▊    | 221M/380M [00:03<00:02, 60.4MB/s]#015Downloading:  60%|█████▉    | 227M/380M [00:03<00:02, 58.7MB/s]#015Downloading:  61%|██████▏   | 233M/380M [00:03<00:02, 58.9MB/s]#015Downloading:  63%|██████▎   | 239M/380M [00:04<00:02, 56.9MB/s]#015Downloading:  65%|██████▍   | 247M/380M [00:04<00:02, 61.7MB/s]#015Downloading:  67%|██████▋   | 255M/380M [00:04<00:01, 67.5MB/s]#015Downloading:  69%|██████▉   | 263M/380M [00:04<00:01, 68.6MB/s]#015Downloading:  71%|███████   | 271M/380M [00:04<00:01, 71.7MB/s]#015Downloading:  73%|███████▎  | 279M/380M [00:04<00:01, 75.1MB/s]#015Downloading:  76%|███████▌  | 288M/380M [00:04<00:01, 77.9MB/s]#015Downloading:  78%|███████▊  | 296M/380M [00:04<00:01, 77.1MB/s]#015Downloading:  80%|███████▉  | 304M/380M [00:04<00:00, 78.0MB/s]#015Downloading:  82%|████████▏ | 311M/380M [00:04<00:00, 75.1MB/s]#015Downloading:  84%|████████▍ | 319M/380M [00:05<00:00, 70.8MB/s]#015Downloading:  86%|████████▌ | 326M/380M [00:05<00:01, 47.3MB/s]#015Downloading:  88%|████████▊ | 333M/380M [00:05<00:00, 52.9MB/s]#015Downloading:  90%|████████▉ | 341M/380M [00:05<00:00, 58.4MB/s]#015Downloading:  92%|█████████▏| 349M/380M [00:05<00:00, 63.7MB/s]#015Downloading:  94%|█████████▍| 357M/380M [00:05<00:00, 68.1MB/s]#015Downloading:  96%|█████████▌| 366M/380M [00:05<00:00, 71.6MB/s]#015Downloading:  98%|█████████▊| 373M/380M [00:05<00:00, 73.1MB/s]#015Downloading: 100%|██████████| 380M/380M [00:06<00:00, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_q.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/36090 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"train.py\", line 194, in <module>\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1272, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1732, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1766, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1078, in forward\n",
      "    zero_infinity=self.config.ctc_zero_infinity,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/smdebug.py\", line 72, in run\n",
      "    return_value = function(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\", line 2201, in ctc_loss\n",
      "    zero_infinity)\u001b[0m\n",
      "\u001b[34mRuntimeError: blank must be in label range\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/36090 [00:10<?, ?it/s]\u001b[0m\n",
      "\u001b[34m2022-10-08 08:00:25,549 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.6 train.py --epochs 10 --fp16 True --hub_model_id fine-tune-xlsr-53-wav2vec2-on-swahili-sagemaker --hub_strategy all_checkpoints --hub_token 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 --model_name facebook/wav2vec2-base --push_to_hub True --train_batch_size 8 --vocab_url s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-blog/vocab.json\"\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.95k [00:00<?, ?B/s]#015Downloading: 4.55kB [00:00, 3.12MB/s]                   \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.84k [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 1.84k/1.84k [00:00<00:00, 1.20MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/380M [00:00<?, ?B/s]#015Downloading:   1%|          | 4.64M/380M [00:00<00:08, 46.4MB/s]#015Downloading:   3%|â         | 9.83M/380M [00:00<00:07, 47.9MB/s]#015Downloading:   4%|â         | 16.3M/380M [00:00<00:07, 51.9MB/s]#015Downloading:   6%|â         | 22.4M/380M [00:00<00:06, 54.4MB/s]#015Downloading:   8%|â         | 30.0M/380M [00:00<00:05, 59.5MB/s]#015Downloading:  10%|â         | 37.3M/380M [00:00<00:05, 62.8MB/s]#015Downloading:  12%|ââ        | 44.9M/380M [00:00<00:05, 66.4MB/s]#015Downloading:  14%|ââ        | 52.9M/380M [00:00<00:04, 69.9MB/s]#015Downloading:  16%|ââ        | 60.4M/380M [00:00<00:04, 71.5MB/s]#015Downloading:  18%|ââ        | 67.5M/380M [00:01<00:04, 68.5MB/s]#015Downloading:  20%|ââ        | 74.9M/380M [00:01<00:04, 70.1MB/s]#015Downloading:  22%|âââ       | 81.9M/380M [00:01<00:04, 69.2MB/s]#015Downloading:  23%|âââ       | 88.8M/380M [00:01<00:06, 46.1MB/s]#015Downloading:  25%|âââ       | 94.5M/380M [00:01<00:06, 42.5MB/s]#015Downloading:  26%|âââ       | 99.7M/380M [00:01<00:06, 45.0MB/s]#015Downloading:  28%|âââ       | 106M/380M [00:01<00:05, 48.6MB/s] #015Downloading:  29%|âââ       | 112M/380M [00:01<00:05, 51.5MB/s]#015Downloading:  31%|âââ       | 117M/380M [00:02<00:05, 51.5MB/s]#015Downloading:  32%|ââââ      | 123M/380M [00:02<00:04, 53.1MB/s]#015Downloading:  34%|ââââ      | 128M/380M [00:02<00:04, 52.1MB/s]#015Downloading:  35%|ââââ      | 134M/380M [00:02<00:04, 53.2MB/s]#015Downloading:  37%|ââââ      | 139M/380M [00:02<00:04, 52.5MB/s]#015Downloading:  38%|ââââ      | 146M/380M [00:02<00:04, 56.4MB/s]#015Downloading:  41%|ââââ      | 155M/380M [00:02<00:03, 62.6MB/s]#015Downloading:  42%|âââââ     | 161M/380M [00:02<00:03, 64.2MB/s]#015Downloading:  44%|âââââ     | 168M/380M [00:02<00:03, 62.9MB/s]#015Downloading:  46%|âââââ     | 175M/380M [00:02<00:03, 62.1MB/s]#015Downloading:  48%|âââââ     | 182M/380M [00:03<00:03, 65.7MB/s]#015Downloading:  50%|âââââ     | 189M/380M [00:03<00:02, 64.7MB/s]#015Downloading:  51%|ââââââ    | 196M/380M [00:03<00:02, 65.4MB/s]#015Downloading:  53%|ââââââ    | 202M/380M [00:03<00:02, 62.3MB/s]#015Downloading:  55%|ââââââ    | 209M/380M [00:03<00:02, 62.9MB/s]#015Downloading:  57%|ââââââ    | 215M/380M [00:03<00:02, 59.5MB/s]#015Downloading:  58%|ââââââ    | 221M/380M [00:03<00:02, 60.4MB/s]#015Downloading:  60%|ââââââ    | 227M/380M [00:03<00:02, 58.7MB/s]#015Downloading:  61%|âââââââ   | 233M/380M [00:03<00:02, 58.9MB/s]#015Downloading:  63%|âââââââ   | 239M/380M [00:04<00:02, 56.9MB/s]#015Downloading:  65%|âââââââ   | 247M/380M [00:04<00:02, 61.7MB/s]#015Downloading:  67%|âââââââ   | 255M/380M [00:04<00:01, 67.5MB/s]#015Downloading:  69%|âââââââ   | 263M/380M [00:04<00:01, 68.6MB/s]#015Downloading:  71%|âââââââ   | 271M/380M [00:04<00:01, 71.7MB/s]#015Downloading:  73%|ââââââââ  | 279M/380M [00:04<00:01, 75.1MB/s]#015Downloading:  76%|ââââââââ  | 288M/380M [00:04<00:01, 77.9MB/s]#015Downloading:  78%|ââââââââ  | 296M/380M [00:04<00:01, 77.1MB/s]#015Downloading:  80%|ââââââââ  | 304M/380M [00:04<00:00, 78.0MB/s]#015Downloading:  82%|âââââââââ | 311M/380M [00:04<00:00, 75.1MB/s]#015Downloading:  84%|âââââââââ | 319M/380M [00:05<00:00, 70.8MB/s]#015Downloading:  86%|âââââââââ | 326M/380M [00:05<00:01, 47.3MB/s]#015Downloading:  88%|âââââââââ | 333M/380M [00:05<00:00, 52.9MB/s]#015Downloading:  90%|âââââââââ | 341M/380M [00:05<00:00, 58.4MB/s]#015Downloading:  92%|ââââââââââ| 349M/380M [00:05<00:00, 63.7MB/s]#015Downloading:  94%|ââââââââââ| 357M/380M [00:05<00:00, 68.1MB/s]#015Downloading:  96%|ââââââââââ| 366M/380M [00:05<00:00, 71.6MB/s]#015Downloading:  98%|ââââââââââ| 373M/380M [00:05<00:00, 73.1MB/s]#015Downloading: 100%|ââââââââââ| 380M/380M [00:06<00:00, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_q.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/36090 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"train.py\", line 194, in <module>\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1272, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1732, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/trainer.py\", line 1766, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 756, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1078, in forward\n",
      "    zero_infinity=self.config.ctc_zero_infinity,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/smdebug.py\", line 72, in run\n",
      "    return_value = function(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\", line 2201, in ctc_loss\n",
      "    zero_infinity)\u001b[0m\n",
      "\u001b[34mRuntimeError: blank must be in label range\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/36090 [00:10<?, ?it/s]\u001b[0m\n",
      "\n",
      "2022-10-08 08:01:31 Uploading - Uploading generated training model\n",
      "2022-10-08 08:01:31 Failed - Training job failed\n",
      "ProfilerReport-1665214558: IssuesFound\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-wav2vec2-training-3-1665214372: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 train.py --epochs 10 --fp16 True --hub_model_id fine-tune-xlsr-53-wav2vec2-on-swahili-sagemaker --hub_strategy all_checkpoints --hub_token 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 --model_name facebook/wav2vec2-base --push_to_hub True --train_batch_size 8 --vocab_url s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-blog/vocab.json\"\n\rDownloading:   0%|          | 0.00/1.95k [00:00<?, ?B/s]\rDownloading: 4.55kB [00:00, 3.12MB/s]                   \n\rDownloading:   0%|          | 0.00/1.84k [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 1.84k/1.84k [00:00<00:00, 1.20MB/s]\n\rDownloading:   0%|          | 0.00/380M [00:00<?, ?B/s]\rDownloading:   1%|          | 4.64M/380M [00:00<00:08, 46.4MB/s]\rDownloading:   3%|â         | 9.83M/380M [00:00<00:07, 47.9MB/s]\rDownloading:   4%|â         | 16.3M/380M [00:00<00:07, 51.9MB/s]\rDownloading:   6%|â         | 22.4M/380M [00:00<00:06, 54.4MB/s]\rDownload, exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-183d04419c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#Starts the training job using the fit function, training takes approximately 2 hours to complete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path},\n\u001b[0;32m---> 23\u001b[0;31m                           job_name=TRAINING_JOB_NAME)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2202\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3865\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3866\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3867\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3868\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3405\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3406\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3407\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3408\u001b[0m             )\n\u001b[1;32m   3409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-wav2vec2-training-3-1665214372: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 train.py --epochs 10 --fp16 True --hub_model_id fine-tune-xlsr-53-wav2vec2-on-swahili-sagemaker --hub_strategy all_checkpoints --hub_token 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 --model_name facebook/wav2vec2-base --push_to_hub True --train_batch_size 8 --vocab_url s3://fine-tune-xlsr-53-wav2vec2-on-swahili/huggingface-blog/vocab.json\"\n\rDownloading:   0%|          | 0.00/1.95k [00:00<?, ?B/s]\rDownloading: 4.55kB [00:00, 3.12MB/s]                   \n\rDownloading:   0%|          | 0.00/1.84k [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 1.84k/1.84k [00:00<00:00, 1.20MB/s]\n\rDownloading:   0%|          | 0.00/380M [00:00<?, ?B/s]\rDownloading:   1%|          | 4.64M/380M [00:00<00:08, 46.4MB/s]\rDownloading:   3%|â         | 9.83M/380M [00:00<00:07, 47.9MB/s]\rDownloading:   4%|â         | 16.3M/380M [00:00<00:07, 51.9MB/s]\rDownloading:   6%|â         | 22.4M/380M [00:00<00:06, 54.4MB/s]\rDownload, exit code: 1"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH= f's3://{BUCKET}/{PREFIX}/{TRAINING_JOB_NAME}/output/'\n",
    "\n",
    "\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='run_wav2vec2_pretraining_no_trainer.py',\n",
    "                                    source_dir='./sagemaker/pretrain_wav2vec/pytorch',\n",
    "                                    output_path= OUTPUT_PATH, \n",
    "                                    instance_type='ml.g4dn.xlarge',\n",
    "                                    instance_count=1,\n",
    "                                    transformers_version='4.6.1',\n",
    "                                    pytorch_version='1.7.1',\n",
    "                                    py_version='py36',\n",
    "                                    role=ROLE,\n",
    "                                    # use_spot_instances=False,  # Use a spot instance \n",
    "                                    # max_run=3600,  # Max training time\n",
    "                                    # max_wait=3600,  # Max training time + spot waiting time\n",
    "                                    hyperparameters = hyperparameters\n",
    "                                   )\n",
    "\n",
    "#Starts the training job using the fit function, training takes approximately 2 hours to complete.\n",
    "huggingface_estimator.fit(job_name=TRAINING_JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training logs you can see that, after 10 epochs of training, and model evaluation metrics wer can achieve around 0.32 for the subset of SUPERB dataset. You can increase the number of epochs or use the full dataset to improve the model further. "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "3e4c26631640e9b131893bef564fd60ba2dcf8108f36d999dbaba747635dc18b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
