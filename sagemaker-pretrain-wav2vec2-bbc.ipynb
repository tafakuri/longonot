{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Wav2Vec2 model for speech recognition with Hugging Face and SageMaker\n",
    "\n",
    "## Background\n",
    "\n",
    "Wav2Vec2 is a transformer-based architecture for ASR tasks and was released in September 2020. We show its simplified architecture diagram below. For more details, see the [original paper](https://arxiv.org/abs/2006.11477). The model is composed of a multi-layer convolutional network (CNN) as feature extractor, which takes input audio signal and outputs audio representations, also considered as features. They are fed into a transformer network to generate contextualized representations. This part of training can be self-supervised, it means that the transformer can be trained with a mass of unlabeled speech and learn from them. Then the model is fine-tuned on labeled data with Connectionist Temporal Classification (CTC) algorithm for specific ASR tasks. The base model we use in this post is [Wav2Vec2-Base-960h](https://huggingface.co/facebook/wav2vec2-base-960h), it is fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. \n",
    "<img src=\"images/wav2vec2.png\">\n",
    "\n",
    "Connectionist Temporal Classification (CTC) is character-based algorithm. During the training, it’s able to demarcate each character of the transcription in the speech automatically, so the timeframe alignment is not required between audio signal and transcription. For example, one audio clip says “Hello World”, we don’t need to know in which second word “hello” is located. It saves a lot of labeling effort for ASR use cases. If you are interested in how the algorithm works underneath, see [this article](https://distill.pub/2017/ctc/) for more information.  \n",
    "\n",
    "\n",
    "## Notebook Overview \n",
    "\n",
    "In this notebook, we use [SUPERB \n",
    "(Speech processing Universal PERformance Benchmark) dataset](https://huggingface.co/datasets/superb) that available from Hugging Face Datasets library, and fine-tune the Wav2Vec2 model and deploy it as SageMaker endpoint for real-time inference for an ASR task. \n",
    "<img src=\"images/solution_overview.png\">\n",
    "\n",
    "First of all, we show how to load and preprocess the SUPERB dataset in SageMaker environment in order to obtain tokenizer and feature extractor, which are required for fine-tuning the Wav2Vec2 model. Then we use SageMaker Script Mode for training and inference steps, that allows you to define and use custom training and inference scripts and SageMaker provides supported Hugging Face framework Docker containers. For more information about training and serving Hugging Face models on SageMaker, see Use [Hugging Face with Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html). This functionality is available through the development of Hugging Face [AWS Deep Learning Container (DLC)](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.html). \n",
    "\n",
    "This notebook is tested in both SageMaker Studio and SageMaker Notebook environments. Below shows detailed setup.   \n",
    "- SageMaker Studio: **ml.m5.xlarge** instance with **Data Science** kernel.\n",
    "- SageMaker Notebook: **ml.m5.xlarge** instance with **conda_python3** kernel. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up \n",
    "First, install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.107.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.116.0.tar.gz (592 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m592.4/592.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (21.4.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.24.62)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.20.1)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (4.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.3.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.9)\n",
      "Collecting schema\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.62 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.62)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (4.3.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.13)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.5.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.7.6.5)\n",
      "Requirement already satisfied: pox>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.1)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.62->boto3<2.0,>=1.20.21->sagemaker) (1.26.12)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.116.0-py2.py3-none-any.whl size=809070 sha256=2ceca2ff4ae55c8ffdcd95646147972a290c9dd83e5d2b054c9ab0475206274c\n",
      "  Stored in directory: /root/.cache/pip/wheels/69/db/16/aa0df031157d9bf5793826b6ca00f4124047c99e04d80c372a\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: schema, sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.107.0\n",
      "    Uninstalling sagemaker-2.107.0:\n",
      "      Successfully uninstalled sagemaker-2.107.0\n",
      "Successfully installed sagemaker-2.116.0 schema-0.7.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting transformers>=4.4.2\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (4.42.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (1.21.6)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (2.28.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (4.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.2) (2022.8.17)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m766.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.4.2) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers>=4.4.2) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.4.2) (3.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.4.2) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.4.2) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.4.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.4.2) (2.8)\n",
      "Installing collected packages: tokenizers, packaging, huggingface-hub, transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.1\n",
      "    Uninstalling packaging-20.1:\n",
      "      Successfully uninstalled packaging-20.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.10.1 packaging-21.3 tokenizers-0.13.1 transformers-4.23.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (2022.7.1)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.10.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from s3fs) (3.8.1)\n",
      "Collecting aiobotocore~=2.4.0\n",
      "  Downloading aiobotocore-2.4.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec==2022.10.0\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore~=2.4.0->s3fs) (1.11.2)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore~=2.4.0->s3fs) (0.10.0)\n",
      "Collecting botocore<1.27.60,>=1.27.59\n",
      "  Downloading botocore-1.27.59-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (21.4.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (1.26.12)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (1.14.0)\n",
      "Installing collected packages: fsspec, botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.62\n",
      "    Uninstalling botocore-1.27.62:\n",
      "      Successfully uninstalled botocore-1.27.62\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 2.3.4\n",
      "    Uninstalling aiobotocore-2.3.4:\n",
      "      Successfully uninstalled aiobotocore-2.3.4\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 2022.7.1\n",
      "    Uninstalling s3fs-2022.7.1:\n",
      "      Successfully uninstalled s3fs-2022.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.24.62 requires botocore<1.28.0,>=1.27.62, but you have botocore 1.27.59 which is incompatible.\n",
      "awscli 1.25.63 requires botocore==1.27.62, but you have botocore 1.27.59 which is incompatible.\n",
      "awscli 1.25.63 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "awscli 1.25.63 requires rsa<4.8,>=3.1.2, but you have rsa 4.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.4.0 botocore-1.27.59 fsspec-2022.10.0 s3fs-2022.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.9/441.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.12.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (2.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: xxhash, tqdm, responses, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.1\n",
      "    Uninstalling tqdm-4.42.1:\n",
      "      Successfully uninstalled tqdm-4.42.1\n",
      "Successfully installed datasets-2.6.1 responses-0.18.0 tqdm-4.64.1 xxhash-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp37-cp37m-manylinux1_x86_64.whl (890.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.2/890.2 MB\u001b[0m \u001b[31m971.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.3.0)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.13.0-cp37-cp37m-manylinux1_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch==1.13.0 in /opt/conda/lib/python3.7/site-packages (from torchaudio) (1.13.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->torchaudio) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->torchaudio) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->torchaudio) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->torchaudio) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->torchaudio) (4.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchaudio) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchaudio) (59.3.0)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.13.0-py2.py3-none-any.whl (174 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (2.28.1)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/232.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.50.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (59.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (0.34.2)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.21.6)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.14.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard) (4.12.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, werkzeug, tensorboard-data-server, pyasn1-modules, protobuf, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.0\n",
      "    Uninstalling Werkzeug-1.0.0:\n",
      "      Successfully uninstalled Werkzeug-1.0.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "Successfully installed absl-py-1.3.0 cachetools-5.2.0 google-auth-2.13.0 google-auth-oauthlib-0.4.6 grpcio-1.50.0 markdown-3.4.1 oauthlib-3.2.2 protobuf-3.19.6 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 werkzeug-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): - "
     ]
    }
   ],
   "source": [
    "!pip install sagemaker --upgrade\n",
    "!pip install \"transformers>=4.4.2\" \n",
    "!pip install s3fs --upgrade\n",
    "!pip install datasets --upgrade \n",
    "#!pip install \"librosa==0.9.1librosa\"\n",
    "!pip install torch # framework is required for transformer \n",
    "!pip install torchaudio\n",
    "!pip install transformers\n",
    "!pip install accelerate>=0.5.0\n",
    "!pip install tensorboard\n",
    "!pip install wandb\n",
    "\n",
    "\n",
    "!conda install -y -c conda-forge librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**soundfile** library will be used to read raw audio files and convert them into arrays. Before installing **soundfile** python library, package **libsndfile** needs to be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: done\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - libsndfile\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2022.9.24  |       ha878542_0         150 KB  conda-forge\n",
      "    certifi-2022.9.24          |     pyhd8ed1ab_0         155 KB  conda-forge\n",
      "    conda-22.9.0               |   py37h89c1867_1         960 KB  conda-forge\n",
      "    gettext-0.19.8.1           |       h5e8e0c9_1         3.5 MB  conda-forge\n",
      "    libflac-1.3.3              |       he1b5a44_0         517 KB  conda-forge\n",
      "    libogg-1.3.2               |    h516909a_1002         206 KB  conda-forge\n",
      "    libsndfile-1.0.29          |       he1b5a44_0         534 KB  conda-forge\n",
      "    libvorbis-1.3.7            |       he1b5a44_0         287 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         6.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  gettext            conda-forge/linux-64::gettext-0.19.8.1-h5e8e0c9_1\n",
      "  libflac            conda-forge/linux-64::libflac-1.3.3-he1b5a44_0\n",
      "  libogg             conda-forge/linux-64::libogg-1.3.2-h516909a_1002\n",
      "  libsndfile         conda-forge/linux-64::libsndfile-1.0.29-he1b5a44_0\n",
      "  libvorbis          conda-forge/linux-64::libvorbis-1.3.7-he1b5a44_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2022.6.15-ha878542_0 --> 2022.9.24-ha878542_0\n",
      "  certifi                            2022.6.15-pyhd8ed1ab_1 --> 2022.9.24-pyhd8ed1ab_0\n",
      "  conda                               4.14.0-py37h89c1867_0 --> 22.9.0-py37h89c1867_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "libflac-1.3.3        | 517 KB    | ##################################### | 100% \n",
      "libogg-1.3.2         | 206 KB    | ##################################### | 100% \n",
      "gettext-0.19.8.1     | 3.5 MB    | ##################################### | 100% \n",
      "conda-22.9.0         | 960 KB    | ##################################### | 100% \n",
      "libsndfile-1.0.29    | 534 KB    | ##################################### | 100% \n",
      "ca-certificates-2022 | 150 KB    | ##################################### | 100% \n",
      "certifi-2022.9.24    | 155 KB    | ##################################### | 100% \n",
      "libvorbis-1.3.7      | 287 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Retrieving notices: ...working... done\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile) (2.19)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge libsndfile -y\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: librosa 0.9.2\n",
      "Uninstalling librosa-0.9.2:\n",
      "  Successfully uninstalled librosa-0.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall -y librosa\n",
    "#!conda install -y -c conda-forge librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Following let's import common python libraries. Create a S3 bucket in AWS console for this project, and replace **[BUCKET_NAME]** with your bucket. \n",
    "Get the execution role which allows training and servering jobs to access your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::121713061542:role/service-role/AmazonSageMaker-ExecutionRole-20220927T193257\n",
      "sagemaker bucket: pretrain-wav2vec2-on-swahili\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import random\n",
    "import soundfile \n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "\n",
    "BUCKET=\"pretrain-wav2vec2-on-swahili\" # please use your bucket name\n",
    "PREFIX = \"10h-dataset\" \n",
    "ROLE = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=BUCKET)\n",
    "\n",
    "print(f\"sagemaker role arn: {ROLE}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d5508dd6e746709ef693ae6c82a15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.sagemaker_auth(path=\"./sagemaker/pretrain_wav2vec/pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "We are using SUPERB dataset for this notebook, which can be loaded from Hugging Face [dataset library](https://huggingface.co/datasets/superb) directly using `load_dataset` function. SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. It also includes speaker_id and chapter_id etc., these columns are removed from the dataset, and we only keep audio files and transcriptions to fine-tune the Wav2Vec2 model for an audio recognition task, which transcribes speech to text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "HF_API_TOKEN=HfFolder.get_token()\n",
    "HF_MODEL_ID=\"mutisya/wav2vec2-pretrain-bbc-sage-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the HuggingFace model (Wav2Vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training script\n",
    "\n",
    "Here we are using SageMaker HuggingFace DLC (Deep Learning Container) script mode to construct the training and inference job, which allows you to write custom trianing and serving code and using HuggingFace framework containers that maintained and supported by AWS. \n",
    "\n",
    "When we create a training job using the script mode, the `entry_point` script, hyperparameters, its dependencies (inside requirements.txt) and input data (train and test datasets) will be copied into the container. Then it invokes the `entry_point` training script, where the train and test datasets will be loaded, training steps will be executed and model artifacts will be saved in `/opt/ml/model` in the container. After training, artifacts in this directory are uploaded to S3 for later model hosting.\n",
    "\n",
    "This script is saved in directory `scripts`, and you can inspect the training script by running the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    Wav2Vec2ForCTC, \n",
      "    Trainer, \n",
      "    TrainingArguments, \n",
      "    Wav2Vec2CTCTokenizer, \n",
      "    Wav2Vec2FeatureExtractor, \n",
      "    Wav2Vec2Processor)\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk, load_metric\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdataclasses\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m dataclass\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dict, List, Optional, Union\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      " \n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m8\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m8\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mfacebook/wav2vec2-base\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m1e-4\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--weight_decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m0.005\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--vocab_url\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[37m# set up logging\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "    logging.basicConfig(\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(args.training_dir)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args.test_dir)\n",
      "    train_dataset = load_from_disk(args.training_dir)\n",
      "    test_dataset = load_from_disk(args.test_dir)\n",
      "\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(train_dataset)}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mlen(test_dataset)}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "\n",
      "    \u001b[37m# add tokenizer, feature extractor and processor \u001b[39;49;00m\n",
      "    s3 = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    vocab_url = args.vocab_url\n",
      "    s3.download_file(vocab_url.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m2\u001b[39;49;00m],vocab_url.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m3\u001b[39;49;00m]+\u001b[33m'\u001b[39;49;00m\u001b[33m/vocab.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mvocab.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    tokenizer = Wav2Vec2CTCTokenizer(\u001b[33m\"\u001b[39;49;00m\u001b[33mvocab.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, unk_token=\u001b[33m\"\u001b[39;49;00m\u001b[33m[UNK]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, pad_token=\u001b[33m\"\u001b[39;49;00m\u001b[33m[PAD]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, word_delimiter_token=\u001b[33m\"\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \n",
      "    feature_extractor = Wav2Vec2FeatureExtractor(feature_size=\u001b[34m1\u001b[39;49;00m, sampling_rate=\u001b[34m16000\u001b[39;49;00m, padding_value=\u001b[34m0.0\u001b[39;49;00m, do_normalize=\u001b[34mTrue\u001b[39;49;00m, return_attention_mask=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
      "\n",
      "    \u001b[37m# the data collator class is for dynamic padding and speed up data processing in CPU GPU. The code is from: \u001b[39;49;00m\n",
      "    \u001b[37m# https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81\u001b[39;49;00m\n",
      "    \u001b[90m@dataclass\u001b[39;49;00m\n",
      "    \u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDataCollatorCTCWithPadding\u001b[39;49;00m:\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Data collator that will dynamically pad the inputs received.\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            processor (:class:`~transformers.Wav2Vec2Processor`)\u001b[39;49;00m\n",
      "\u001b[33m                The processor used for proccessing the data.\u001b[39;49;00m\n",
      "\u001b[33m            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\u001b[39;49;00m\n",
      "\u001b[33m                Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\u001b[39;49;00m\n",
      "\u001b[33m                among:\u001b[39;49;00m\n",
      "\u001b[33m                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\u001b[39;49;00m\n",
      "\u001b[33m                  sequence if provided).\u001b[39;49;00m\n",
      "\u001b[33m                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\u001b[39;49;00m\n",
      "\u001b[33m                  maximum acceptable input length for the model if that argument is not provided.\u001b[39;49;00m\n",
      "\u001b[33m                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\u001b[39;49;00m\n",
      "\u001b[33m                  different lengths).\u001b[39;49;00m\n",
      "\u001b[33m            max_length (:obj:`int`, `optional`):\u001b[39;49;00m\n",
      "\u001b[33m                Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\u001b[39;49;00m\n",
      "\u001b[33m            max_length_labels (:obj:`int`, `optional`):\u001b[39;49;00m\n",
      "\u001b[33m                Maximum length of the ``labels`` returned list and optionally padding length (see above).\u001b[39;49;00m\n",
      "\u001b[33m            pad_to_multiple_of (:obj:`int`, `optional`):\u001b[39;49;00m\n",
      "\u001b[33m                If set will pad the sequence to a multiple of the provided value.\u001b[39;49;00m\n",
      "\u001b[33m                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\u001b[39;49;00m\n",
      "\u001b[33m                7.5 (Volta).\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        processor: Wav2Vec2Processor\n",
      "        padding: Union[\u001b[36mbool\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m] = \u001b[34mTrue\u001b[39;49;00m\n",
      "        max_length: Optional[\u001b[36mint\u001b[39;49;00m] = \u001b[34mNone\u001b[39;49;00m\n",
      "        max_length_labels: Optional[\u001b[36mint\u001b[39;49;00m] = \u001b[34mNone\u001b[39;49;00m\n",
      "        pad_to_multiple_of: Optional[\u001b[36mint\u001b[39;49;00m] = \u001b[34mNone\u001b[39;49;00m\n",
      "        pad_to_multiple_of_labels: Optional[\u001b[36mint\u001b[39;49;00m] = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32m__call__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, features: List[Dict[\u001b[36mstr\u001b[39;49;00m, Union[List[\u001b[36mint\u001b[39;49;00m], torch.Tensor]]]) -> Dict[\u001b[36mstr\u001b[39;49;00m, torch.Tensor]:\n",
      "            \u001b[37m# split inputs and labels since they have to be of different lenghts and need\u001b[39;49;00m\n",
      "            \u001b[37m# different padding methods\u001b[39;49;00m\n",
      "            input_features = [{\u001b[33m\"\u001b[39;49;00m\u001b[33minput_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: feature[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]} \u001b[34mfor\u001b[39;49;00m feature \u001b[35min\u001b[39;49;00m features]\n",
      "            label_features = [{\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: feature[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]} \u001b[34mfor\u001b[39;49;00m feature \u001b[35min\u001b[39;49;00m features]\n",
      "\n",
      "            batch = \u001b[36mself\u001b[39;49;00m.processor.pad(\n",
      "                input_features,\n",
      "                padding=\u001b[36mself\u001b[39;49;00m.padding,\n",
      "                max_length=\u001b[36mself\u001b[39;49;00m.max_length,\n",
      "                pad_to_multiple_of=\u001b[36mself\u001b[39;49;00m.pad_to_multiple_of,\n",
      "                return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            )\n",
      "            \u001b[34mwith\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.processor.as_target_processor():\n",
      "                labels_batch = \u001b[36mself\u001b[39;49;00m.processor.pad(\n",
      "                    label_features,\n",
      "                    padding=\u001b[36mself\u001b[39;49;00m.padding,\n",
      "                    max_length=\u001b[36mself\u001b[39;49;00m.max_length_labels,\n",
      "                    pad_to_multiple_of=\u001b[36mself\u001b[39;49;00m.pad_to_multiple_of_labels,\n",
      "                    return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                )\n",
      "\n",
      "            \u001b[37m# replace padding with -100 to ignore loss correctly\u001b[39;49;00m\n",
      "            labels = labels_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].masked_fill(labels_batch.attention_mask.ne(\u001b[34m1\u001b[39;49;00m), -\u001b[34m100\u001b[39;49;00m)\n",
      "\n",
      "            batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = labels\n",
      "\n",
      "            \u001b[34mreturn\u001b[39;49;00m batch\n",
      "    \n",
      "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# compute metrics\u001b[39;49;00m\n",
      "    wer_metric = load_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33mwer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred):\n",
      "        pred_logits = pred.predictions\n",
      "        pred_ids = np.argmax(pred_logits, axis=-\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "        pred.label_ids[pred.label_ids == -\u001b[34m100\u001b[39;49;00m] = processor.tokenizer.pad_token_id\n",
      "\n",
      "        pred_str = processor.batch_decode(pred_ids)\n",
      "        label_str = processor.batch_decode(pred.label_ids, group_tokens=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mwer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: wer}\n",
      "\n",
      "\n",
      "    model = Wav2Vec2ForCTC.from_pretrained(\n",
      "        args.model_name, \n",
      "        gradient_checkpointing=\u001b[34mTrue\u001b[39;49;00m, \n",
      "        ctc_loss_reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "        pad_token_id=processor.tokenizer.pad_token_id,\n",
      "    )\n",
      "    model.freeze_feature_extractor()\n",
      "\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=args.model_dir,\n",
      "        group_by_length=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        per_device_train_batch_size=args.train_batch_size,\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        num_train_epochs=args.epochs,\n",
      "        fp16=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m# enable mixed-precision training\u001b[39;49;00m\n",
      "        save_steps=\u001b[34m50\u001b[39;49;00m,\n",
      "        eval_steps=\u001b[34m50\u001b[39;49;00m,\n",
      "        logging_steps=\u001b[34m50\u001b[39;49;00m,\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\n",
      "        weight_decay=\u001b[36mfloat\u001b[39;49;00m(args.weight_decay),\n",
      "        warmup_steps=args.warmup_steps,\n",
      "        save_total_limit=\u001b[34m2\u001b[39;49;00m,\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{args.output_data_dir}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        data_collator=data_collator,\n",
      "        args=training_args,\n",
      "        compute_metrics=compute_metrics,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=test_dataset,\n",
      "        tokenizer=processor.feature_extractor,\n",
      "    )\n",
      "    \n",
      "    \u001b[37m# train model\u001b[39;49;00m\n",
      "    trainer.train()\n",
      "\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{key}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{value}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir)\n",
      "    tokenizer.save_pretrained(args.model_dir)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize sagemaker/pretrain_wav2vec/pytorch/run_wav2vec2_pretraining_no_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Estimator and start a training job\n",
    "\n",
    "Worth to highlight that, when you create a Hugging Face Estimator, you can configure hyperparameters and provide a custom parameter into the training script, such as `vocab_url` in this example. Also you can specify the metrics in the Estimator, and parse the logs of metrics and send them to CloudWatch to monitor and track the training performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name:  huggingface-wav2vec2-pretrain-bbc-1667086094\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "#create an unique id to tag training job, model name and endpoint name. \n",
    "id = int(time.time())\n",
    "\n",
    "TRAINING_JOB_NAME = f\"huggingface-wav2vec2-pretrain-bbc-{id}\"\n",
    "print('Training job name: ', TRAINING_JOB_NAME)\n",
    "\n",
    "vocab_url = f\"s3://{BUCKET}/{PREFIX}/vocab.json\"\n",
    "hyperparameters = {\n",
    "    'dataset_name':\"mutisya/bbc_v0\",\n",
    "    'dataset_split_names': \"train\", \n",
    "    'dataset_config_names': \"train\", \n",
    "    'dataset_use_auth_token' : \"True\",\n",
    "    'model_name_or_path': \"patrickvonplaten/wav2vec2-base-v2\",\n",
    "    'output_dir': \"./wav2vec2-pretrain-bbc-demo-4\",\n",
    "    'max_train_steps': \"20000\",\n",
    "    'num_warmup_steps': \"32000\",\n",
    "    'saving_steps': \"10000\",\n",
    "    'gradient_accumulation_steps': \"4\",\n",
    "    'learning_rate': \"0.002\",\n",
    "    'weight_decay' : \"0.01\",\n",
    "    'max_duration_in_seconds': \"20.0\",\n",
    "    'min_duration_in_seconds' : \"2.0\",\n",
    "    'logging_steps': \"1\",\n",
    "    'per_device_train_batch_size' : \"4\",\n",
    "    'per_device_eval_batch_size': \"4\",\n",
    "    'adam_beta1': \"0.9\",\n",
    "    'adam_beta2' : \"0.98\",\n",
    "    'adam_epsilon' : \"1e-06\",\n",
    "    'push_to_hub': \"False\",\n",
    "    'hub_token': HF_API_TOKEN,\n",
    "    'hub_model_id': HF_MODEL_ID,\n",
    "  }\n",
    "\n",
    "# define metrics definitions\n",
    "metric_definitions=[\n",
    "        {'Name': 'val_loss', 'Regex': \"'val_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'val_contrastive_loss', 'Regex': \"'val_contrastive_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'val_diversity_loss', 'Regex': \"'val_diversity_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'val_num_losses', 'Regex': \"'val_num_losses': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'contrast_loss', 'Regex': \"'contrast_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'div_loss', 'Regex': \"'div_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': '%_mask_idx', 'Regex': \"'%_mask_idx': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'ppl', 'Regex': \"'ppl': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'lr', 'Regex': \"'lr': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'temp', 'Regex': \"'temp': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'grad_norm', 'Regex': \"'grad_norm': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name:  huggingface-wav2vec2-pretrain-bbc-1667079536\n"
     ]
    }
   ],
   "source": [
    "#create an unique id to tag training job, model name and endpoint name. \n",
    "id = int(time.time())\n",
    "\n",
    "TRAINING_JOB_NAME = f\"huggingface-wav2vec2-pretrain-bbc-{id}\"\n",
    "print('Training job name: ', TRAINING_JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [HuggingFace estimator class](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) to train our model. When creating the estimator, the following parameters need to specify. \n",
    "\n",
    "* **entry_point**: the name of the training script. It loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model. \n",
    "* **source_dir**: the location of the training scripts. \n",
    "* **transformers_version**: the Hugging Face transformers library version we want to use.\n",
    "* **pytorch_version**: the pytorch version that compatible with transformers library. \n",
    "\n",
    "**Instance Selection**: For this use case and dataset, we use one ml.p3.2xlarge instance and the training job is able to finish within two hours. You can select a more powerful instance to reduce the training time, however it will generate more cost.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-29 23:28:18 Starting - Starting the training job...\n",
      "2022-10-29 23:28:42 Starting - Preparing the instances for trainingProfilerReport-1667086097: InProgress\n",
      "......\n",
      "2022-10-29 23:29:44 Downloading - Downloading input data...\n",
      "2022-10-29 23:30:15 Training - Downloading the training image..........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-10-29 23:34:30,717 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-10-29 23:34:30,743 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-10-29 23:34:30,750 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-10-29 23:34:31,200 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.18.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.5 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.10.2+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchaudio in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (0.10.2+cu113)\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.5.0\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.13.2-py3-none-any.whl (148 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.8/148.8 kB 4.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: librosa in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.9.2)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 66.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (4.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface_hub>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.10.0)\u001b[0m\n",
      "\u001b[34mCollecting setuptools==59.5.0\u001b[0m\n",
      "\u001b[34mDownloading setuptools-59.5.0-py3-none-any.whl (952 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 952.4/952.4 kB 67.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wandb\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 73.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (2022.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (4.64.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (3.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.12.0->-r requirements.txt (line 1)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 2)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate>=0.5.0->-r requirements.txt (line 4)) (5.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate>=0.5.0->-r requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.8/site-packages (from librosa->-r requirements.txt (line 5)) (0.53.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa->-r requirements.txt (line 5)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa->-r requirements.txt (line 5)) (1.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa->-r requirements.txt (line 5)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.8/site-packages (from librosa->-r requirements.txt (line 5)) (1.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa->-r requirements.txt (line 5)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.8/site-packages (from librosa->-r requirements.txt (line 5)) (5.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from librosa->-r requirements.txt (line 5)) (1.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.8/site-packages (from librosa->-r requirements.txt (line 5)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 6)) (0.37.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 6)) (2.2.2)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 20.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.50.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 92.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 99.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 67.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 6)) (3.19.5)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.3.0-py3-none-any.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.6/124.6 kB 10.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.13.0-py2.py3-none-any.whl (174 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 174.5/174.5 kB 36.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (2022.9.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (0.0.53)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting GitPython>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.29-py3-none-any.whl (182 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.5/182.5 kB 11.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting shortuuid>=0.5.0\u001b[0m\n",
      "\u001b[34mDownloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting promise<3,>=2.0\u001b[0m\n",
      "\u001b[34mDownloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.6/166.6 kB 34.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb->-r requirements.txt (line 10)) (8.1.3)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb->-r requirements.txt (line 10)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.12.0->-r requirements.txt (line 1)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.12.0->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.12.0->-r requirements.txt (line 1)) (2.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.12.0->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.12.0->-r requirements.txt (line 1)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.12.0->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.12.0->-r requirements.txt (line 1)) (1.8.1)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.9-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.1/63.1 kB 14.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 33.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 6)) (4.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.45.1->librosa->-r requirements.txt (line 5)) (0.36.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=1.12.0->-r requirements.txt (line 1)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa->-r requirements.txt (line 5)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.12.0->-r requirements.txt (line 1)) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.12.0->-r requirements.txt (line 1)) (1.26.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.12.0->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.19.1->librosa->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa->-r requirements.txt (line 5)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.12.0->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.12.0->-r requirements.txt (line 1)) (2022.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa->-r requirements.txt (line 5)) (2.21)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 6)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 33.2 MB/s eta 0:00:00\u001b[0m\n",
      "\n",
      "2022-10-29 23:34:45 Training - Training image download completed. Training in progress.\u001b[34mBuilding wheels for collected packages: promise, pathtools\u001b[0m\n",
      "\u001b[34mBuilding wheel for promise (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for promise (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=422cca0f1528bb4e1f5352f3c7e76813b4496e9cc286a3fe32810c3240582c1e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8792 sha256=37044650e9e6a69f460d749097e0d31c65829b5cf3b21d76e61e6e8ad363ed6c\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\u001b[0m\n",
      "\u001b[34mSuccessfully built promise pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboard-plugin-wit, pathtools, tensorboard-data-server, smmap, shortuuid, setuptools, setproctitle, sentry-sdk, pyasn1-modules, promise, oauthlib, grpcio, docker-pycreds, cachetools, absl-py, requests-oauthlib, markdown, google-auth, gitdb, accelerate, google-auth-oauthlib, GitPython, wandb, tensorboard\u001b[0m\n",
      "\u001b[34mAttempting uninstall: setuptools\u001b[0m\n",
      "\u001b[34mFound existing installation: setuptools 65.4.0\u001b[0m\n",
      "\u001b[34mUninstalling setuptools-65.4.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled setuptools-65.4.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.29 absl-py-1.3.0 accelerate-0.13.2 cachetools-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 google-auth-2.13.0 google-auth-oauthlib-0.4.6 grpcio-1.50.0 markdown-3.4.1 oauthlib-3.2.2 pathtools-0.1.2 promise-2.3 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 sentry-sdk-1.10.1 setproctitle-1.3.2 setuptools-59.5.0 shortuuid-1.0.9 smmap-5.0.0 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 wandb-0.13.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-10-29 23:34:45,329 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-10-29 23:34:45,329 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-10-29 23:34:45,410 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.98\",\n",
      "        \"adam_epsilon\": \"1e-06\",\n",
      "        \"dataset_config_names\": \"train\",\n",
      "        \"dataset_name\": \"mutisya/bbc_v0\",\n",
      "        \"dataset_split_names\": \"train\",\n",
      "        \"dataset_use_auth_token\": \"True\",\n",
      "        \"gradient_accumulation_steps\": \"4\",\n",
      "        \"hub_model_id\": \"mutisya/wav2vec2-pretrain-bbc-sage-1\",\n",
      "        \"hub_token\": \"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\",\n",
      "        \"learning_rate\": \"0.002\",\n",
      "        \"logging_steps\": \"1\",\n",
      "        \"max_duration_in_seconds\": \"20.0\",\n",
      "        \"max_train_steps\": \"20000\",\n",
      "        \"min_duration_in_seconds\": \"2.0\",\n",
      "        \"model_name_or_path\": \"patrickvonplaten/wav2vec2-base-v2\",\n",
      "        \"num_warmup_steps\": \"32000\",\n",
      "        \"output_dir\": \"./wav2vec2-pretrain-bbc-demo-4\",\n",
      "        \"per_device_eval_batch_size\": \"4\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"push_to_hub\": \"False\",\n",
      "        \"saving_steps\": \"10000\",\n",
      "        \"weight_decay\": \"0.01\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"huggingface-wav2vec2-pretrain-bbc-1667086094\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://pretrain-wav2vec2-on-swahili/huggingface-wav2vec2-pretrain-bbc-1667086094/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_wav2vec2_pretraining_no_trainer_sagemaker\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_wav2vec2_pretraining_no_trainer_sagemaker.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.98\",\"adam_epsilon\":\"1e-06\",\"dataset_config_names\":\"train\",\"dataset_name\":\"mutisya/bbc_v0\",\"dataset_split_names\":\"train\",\"dataset_use_auth_token\":\"True\",\"gradient_accumulation_steps\":\"4\",\"hub_model_id\":\"mutisya/wav2vec2-pretrain-bbc-sage-1\",\"hub_token\":\"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\",\"learning_rate\":\"0.002\",\"logging_steps\":\"1\",\"max_duration_in_seconds\":\"20.0\",\"max_train_steps\":\"20000\",\"min_duration_in_seconds\":\"2.0\",\"model_name_or_path\":\"patrickvonplaten/wav2vec2-base-v2\",\"num_warmup_steps\":\"32000\",\"output_dir\":\"./wav2vec2-pretrain-bbc-demo-4\",\"per_device_eval_batch_size\":\"4\",\"per_device_train_batch_size\":\"4\",\"push_to_hub\":\"False\",\"saving_steps\":\"10000\",\"weight_decay\":\"0.01\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_wav2vec2_pretraining_no_trainer_sagemaker.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_wav2vec2_pretraining_no_trainer_sagemaker\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://pretrain-wav2vec2-on-swahili/huggingface-wav2vec2-pretrain-bbc-1667086094/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.98\",\"adam_epsilon\":\"1e-06\",\"dataset_config_names\":\"train\",\"dataset_name\":\"mutisya/bbc_v0\",\"dataset_split_names\":\"train\",\"dataset_use_auth_token\":\"True\",\"gradient_accumulation_steps\":\"4\",\"hub_model_id\":\"mutisya/wav2vec2-pretrain-bbc-sage-1\",\"hub_token\":\"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\",\"learning_rate\":\"0.002\",\"logging_steps\":\"1\",\"max_duration_in_seconds\":\"20.0\",\"max_train_steps\":\"20000\",\"min_duration_in_seconds\":\"2.0\",\"model_name_or_path\":\"patrickvonplaten/wav2vec2-base-v2\",\"num_warmup_steps\":\"32000\",\"output_dir\":\"./wav2vec2-pretrain-bbc-demo-4\",\"per_device_eval_batch_size\":\"4\",\"per_device_train_batch_size\":\"4\",\"push_to_hub\":\"False\",\"saving_steps\":\"10000\",\"weight_decay\":\"0.01\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"huggingface-wav2vec2-pretrain-bbc-1667086094\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://pretrain-wav2vec2-on-swahili/huggingface-wav2vec2-pretrain-bbc-1667086094/source/sourcedir.tar.gz\",\"module_name\":\"run_wav2vec2_pretraining_no_trainer_sagemaker\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_wav2vec2_pretraining_no_trainer_sagemaker.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.98\",\"--adam_epsilon\",\"1e-06\",\"--dataset_config_names\",\"train\",\"--dataset_name\",\"mutisya/bbc_v0\",\"--dataset_split_names\",\"train\",\"--dataset_use_auth_token\",\"True\",\"--gradient_accumulation_steps\",\"4\",\"--hub_model_id\",\"mutisya/wav2vec2-pretrain-bbc-sage-1\",\"--hub_token\",\"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\",\"--learning_rate\",\"0.002\",\"--logging_steps\",\"1\",\"--max_duration_in_seconds\",\"20.0\",\"--max_train_steps\",\"20000\",\"--min_duration_in_seconds\",\"2.0\",\"--model_name_or_path\",\"patrickvonplaten/wav2vec2-base-v2\",\"--num_warmup_steps\",\"32000\",\"--output_dir\",\"./wav2vec2-pretrain-bbc-demo-4\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\",\"--push_to_hub\",\"False\",\"--saving_steps\",\"10000\",\"--weight_decay\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.98\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_EPSILON=1e-06\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_CONFIG_NAMES=train\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=mutisya/bbc_v0\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_SPLIT_NAMES=train\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_USE_AUTH_TOKEN=True\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_HUB_MODEL_ID=mutisya/wav2vec2-pretrain-bbc-sage-1\u001b[0m\n",
      "\u001b[34mSM_HP_HUB_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DURATION_IN_SECONDS=20.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_STEPS=20000\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_DURATION_IN_SECONDS=2.0\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=patrickvonplaten/wav2vec2-base-v2\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_WARMUP_STEPS=32000\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=./wav2vec2-pretrain-bbc-demo-4\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PUSH_TO_HUB=False\u001b[0m\n",
      "\u001b[34mSM_HP_SAVING_STEPS=10000\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 run_wav2vec2_pretraining_no_trainer_sagemaker.py --adam_beta1 0.9 --adam_beta2 0.98 --adam_epsilon 1e-06 --dataset_config_names train --dataset_name mutisya/bbc_v0 --dataset_split_names train --dataset_use_auth_token True --gradient_accumulation_steps 4 --hub_model_id mutisya/wav2vec2-pretrain-bbc-sage-1 --hub_token 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 --learning_rate 0.002 --logging_steps 1 --max_duration_in_seconds 20.0 --max_train_steps 20000 --min_duration_in_seconds 2.0 --model_name_or_path patrickvonplaten/wav2vec2-base-v2 --num_warmup_steps 32000 --output_dir ./wav2vec2-pretrain-bbc-demo-4 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --push_to_hub False --saving_steps 10000 --weight_decay 0.01\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: mutisya. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.13.4\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20221029_233454-huggingface-wav2vec2-pretrain-bbc-1667086094-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run huggingface-wav2vec2-pretrain-bbc-1667086094-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/mutisya/wav2vec2-pretrain-bbc-demo-4\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/mutisya/wav2vec2-pretrain-bbc-demo-4/runs/huggingface-wav2vec2-pretrain-bbc-1667086094-algo-1\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"run_wav2vec2_pretraining_no_trainer_sagemaker.py\", line 782, in <module>\n",
      "    main()\u001b[0m\n",
      "\u001b[34mFile \"run_wav2vec2_pretraining_no_trainer_sagemaker.py\", line 425, in main\n",
      "    dataset_split = load_dataset(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1675, in load_dataset\n",
      "    builder_instance = load_dataset_builder(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1512, in load_dataset_builder\n",
      "    dataset_module = dataset_module_factory(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1203, in dataset_module_factory\n",
      "    raise e1 from None\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1175, in dataset_module_factory\n",
      "    raise e\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1155, in dataset_module_factory\n",
      "    dataset_info = hf_api.dataset_info(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 94, in _inner_fn\n",
      "    return fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\", line 98, in inner_f\n",
      "    return f(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py\", line 1358, in dataset_info\n",
      "    hf_raise_for_status(r)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py\", line 242, in hf_raise_for_status\n",
      "    raise RepositoryNotFoundError(message, response) from e\u001b[0m\n",
      "\u001b[34mhuggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: w0rZ9sCxIN-9x6dRindOg)\u001b[0m\n",
      "\u001b[34mRepository Not Found for url: https://huggingface.co/api/datasets/mutisya/bbc_v0.\u001b[0m\n",
      "\u001b[34mPlease make sure you specified the correct `repo_id` and `repo_type`.\u001b[0m\n",
      "\u001b[34mIf the repo is private, make sure you are authenticated.\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.\u001b[0m\n",
      "\u001b[34mwandb: Synced huggingface-wav2vec2-pretrain-bbc-1667086094-algo-1: https://wandb.ai/mutisya/wav2vec2-pretrain-bbc-demo-4/runs/huggingface-wav2vec2-pretrain-bbc-1667086094-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20221029_233454-huggingface-wav2vec2-pretrain-bbc-1667086094-algo-1/logs\u001b[0m\n",
      "\u001b[34m2022-10-29 23:35:01,678 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-10-29 23:35:01,678 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-10-29 23:35:01,678 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2022-10-29 23:35:01,678 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 run_wav2vec2_pretraining_no_trainer_sagemaker.py --adam_beta1 0.9 --adam_beta2 0.98 --adam_epsilon 1e-06 --dataset_config_names train --dataset_name mutisya/bbc_v0 --dataset_split_names train --dataset_use_auth_token True --gradient_accumulation_steps 4 --hub_model_id mutisya/wav2vec2-pretrain-bbc-sage-1 --hub_token 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 --learning_rate 0.002 --logging_steps 1 --max_duration_in_seconds 20.0 --max_train_steps 20000 --min_duration_in_seconds 2.0 --model_name_or_path patrickvonplaten/wav2vec2-base-v2 --num_warmup_steps 32000 --output_dir ./wav2vec2-pretrain-bbc-demo-4 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --push_to_hub False --saving_steps 10000 --weight_decay 0.01\"\u001b[0m\n",
      "\u001b[34m2022-10-29 23:35:01,679 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2022-10-29 23:35:14 Uploading - Uploading generated training model\n",
      "2022-10-29 23:35:14 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-wav2vec2-pretrain-bbc-1667086094: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python3.8 run_wav2vec2_pretraining_no_trainer_sagemaker.py --adam_beta1 0.9 --adam_beta2 0.98 --adam_epsilon 1e-06 --dataset_config_names train --dataset_name mutisya/bbc_v0 --dataset_split_names train --dataset_use_auth_token True --gradient_accumulation_steps 4 --hub_model_id mutisya/wav2vec2-pretrain-bbc-sage-1 --hub_token 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 --learning_rate 0.002 --logging_steps 1 --max_duration_in_seconds 20.0 --max_train_steps 20000 --min_duration_in_seconds 2.0 --model_name_or_path patrickvonplaten/wav2vec2-base-v2 --num_warmup_steps 32000 --output_dir ./wav2vec2-pretrain-bbc-demo-4 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --push_to_hub False --saving_steps 10000 --weight_decay 0.01\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-9daf9823cd89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#Starts the training job using the fit function, training takes approximately 2 hours to complete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAINING_JOB_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3893\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3895\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3435\u001b[0m             )\n\u001b[1;32m   3436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-wav2vec2-pretrain-bbc-1667086094: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python3.8 run_wav2vec2_pretraining_no_trainer_sagemaker.py --adam_beta1 0.9 --adam_beta2 0.98 --adam_epsilon 1e-06 --dataset_config_names train --dataset_name mutisya/bbc_v0 --dataset_split_names train --dataset_use_auth_token True --gradient_accumulation_steps 4 --hub_model_id mutisya/wav2vec2-pretrain-bbc-sage-1 --hub_token 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 --learning_rate 0.002 --logging_steps 1 --max_duration_in_seconds 20.0 --max_train_steps 20000 --min_duration_in_seconds 2.0 --model_name_or_path patrickvonplaten/wav2vec2-base-v2 --num_warmup_steps 32000 --output_dir ./wav2vec2-pretrain-bbc-demo-4 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --push_to_hub False --saving_steps 10000 --weight_decay 0.01\", exit code: 1"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH= f's3://{BUCKET}/{PREFIX}/{TRAINING_JOB_NAME}/output/'\n",
    "\n",
    "env_variables = {\n",
    "    'HF_API_TOKEN':HF_API_TOKEN,\n",
    "    'HF_MODEL_ID': HF_MODEL_ID\n",
    "}\n",
    "huggingface_estimator = HuggingFace(entry_point='run_wav2vec2_pretraining_no_trainer_sagemaker.py',\n",
    "                                    source_dir='./sagemaker/pretrain_wav2vec/pytorch',\n",
    "                                    output_path= OUTPUT_PATH, \n",
    "                                    instance_type='ml.g4dn.xlarge',\n",
    "                                    instance_count=1,\n",
    "                                    transformers_version='4.17.0',\n",
    "                                    pytorch_version='1.10.2',\n",
    "                                    #pytorch_version='1.8.0',\n",
    "                                    py_version='py38',\n",
    "                                    #py_version='py37',\n",
    "                                    role=ROLE,\n",
    "                                    # use_spot_instances=False,  # Use a spot instance \n",
    "                                    max_run=259200,  # Max training time\n",
    "                                    # max_wait=3600,  # Max training time + spot waiting time\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    metric_definitions = metric_definitions,\n",
    "                                    environment = env_variables\n",
    "                                   )\n",
    "\n",
    "#Starts the training job using the fit function, training takes approximately 2 hours to complete.\n",
    "huggingface_estimator.fit(job_name=TRAINING_JOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training logs you can see that, after 10 epochs of training, and model evaluation metrics wer can achieve around 0.32 for the subset of SUPERB dataset. You can increase the number of epochs or use the full dataset to improve the model further. "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "3e4c26631640e9b131893bef564fd60ba2dcf8108f36d999dbaba747635dc18b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
